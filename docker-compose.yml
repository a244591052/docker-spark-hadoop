version: "3"

services:
  namenode:
    image: hadoop:v1
    command: [
      "hdfs",
      "namenode",
      "-Dfs.defaultFS=hdfs://0.0.0.0:9000",
      "-Ddfs.namenode.name.dir=/root/hadoop/dfs/name",
      "-Ddfs.replication=3",
      "-Ddfs.namenode.datanode.registration.ip-hostname-check=false",
      "-Ddfs.permissions.enabled=false",
      "-Ddfs.namenode.safemode.threshold-pct=0"
      ]
    hostname: namenode
    ports:
      - "50070:50070"
    networks:
      - hadoopv1
    volumes:
      - /root/hadoop/dfs/name:/root/hadoop/dfs/name
    deploy:
      placement:
        constraints:
          - node.hostname == master


  datanode:
    image: hadoop:v1
    command: [
      "hdfs",
      "datanode",
      "-fs", "hdfs://namenode:9000",
      "-Ddfs.datanode.data.dir=/root/hadoop/dfs/data",
      "-Ddfs.permissions.enabled=false"
      ]
    deploy:
      replicas: 3
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure
    networks:
      - hadoopv1



  master:
    image: spark:2.2.0
    command: bin/spark-class org.apache.spark.deploy.master.Master -h master
    hostname: master
    environment:
      MASTER: spark://master:7077
      SPARK_MASTER_OPTS: "-Dspark.eventLog.dir=hdfs://namenode:9000/spark/history"
      SPARK_PUBLIC_DNS: master
    ports:
      - 8080:8080
    networks:
      - hadoopv1
    deploy:
      placement:
        constraints:
          - node.hostname == master



  worker:
    image: spark:2.2.0
    command: bin/spark-class org.apache.spark.deploy.worker.Worker spark://master:7077
    environment:
      SPARK_WORKER_CORES: 1
      SPARK_WORKER_MEMORY: 1g
      SPARK_WORKER_PORT: 8881
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_PUBLIC_DNS: master
    networks:
      - hadoopv1
    deploy:
      replicas: 2
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure


  historyServer:
    image: spark:2.2.0
    command: bin/spark-class org.apache.spark.deploy.history.HistoryServer
    hostname: historyServer
    environment:
      MASTER: spark://master:7077
      SPARK_PUBLIC_DNS: master
      SPARK_HISTORY_OPTS: "-Dspark.history.ui.port=18080 -Dspark.history.retainedApplications=3 -Dspark.history.fs.logDirectory=hdfs://namenode:9000/spark/history"
    ports:
      - 18080:18080
    networks:
      - hadoopv1
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.hostname == master


  ResourceManager:
    image: hadoop:v1
    command: [
      "yarn",
      "resourcemanager",
      "-Dfs.defaultFS=hdfs://namenode:9000",
      "-Dyarn.nodemanager.aux-services=mapreduce_shuffle",
      "-Ddfs.namenode.datanode.registration.ip-hostname-check=false",
      "-Ddfs.permissions.enabled=false -Dmapreduce.framework.name=yarn",
      "-Dyarn.resourcemanager.webapp.address=0.0.0.0:8088"
        ]
    hostname: ResourceManager
    ports:
      - "8088:8088"
    networks:
      - hadoopv1
    deploy:
      restart_policy:
        condition: on-failure
      placement:
        constraints:
          - node.hostname == master

  nodemanager:
    image: hadoop:v1
    command: [
      "yarn",
      "nodemanager",
      "-Dyarn.resourcemanager.hostname=ResourceManager",
      "-Dyarn.nodemanager.resource.memory-mb=1024",
      "-Dyarn.nodemanager.aux-services=mapreduce_shuffle",
      "-Ddfs.namenode.datanode.registration.ip-hostname-check=false",
      "-Dmapreduce.framework.name=yarn",
      "-Ddfs.permissions.enabled=false"
      ]
    networks:
      - hadoopv1
    deploy:
      replicas: 3
      update_config:
        parallelism: 2
        delay: 10s
      restart_policy:
        condition: on-failure

networks:
  hadoopv1:
    driver: overlay